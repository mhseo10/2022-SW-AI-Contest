{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 패키지 설치, 데이터셋 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# deep-text-recognition-benchmark의 dependency 설치\n",
    "# !pip install lmdb pillow torchvision nltk natsort fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 대회 목적에 맞게 수정한 ocr 클론\n",
    "# !git clone https://github.com/mhseo10/customocr.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1-1. 데이터셋 압축 해제 경로 지정\n",
    "* 라벨 데이터: `./labels/[dataset_name]/ ... `\n",
    "* 원천 데이터: `./images/[dataset_name]/ ... `\n",
    "* dataset_name:\n",
    "  * 한국어 글자체 데이터셋: `'tiw'`\n",
    "  * 야외 실제 촬영 한글 데이터셋 \n",
    "    * train: `'hub_train'`\n",
    "    * validation: `'hub_valid'`\n",
    "  * 데이콘 데이터셋\n",
    "    * train: `'train'`\n",
    "    * test: `'test'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_timg = './images/hub_train/'\n",
    "hub_tlabel = './labels/hub_train/' \n",
    "\n",
    "hub_vimg = './images/hub_valid/'\n",
    "hub_vlabel = './labels/hub_valid/'\n",
    "\n",
    "train_img = './images/train/'\n",
    "train_label = './labels/train/'\n",
    "\n",
    "tiw_img = './images/tiw/'\n",
    "tiw_label = './labels/tiw/'\n",
    "\n",
    "test_img = './images/test/'\n",
    "\n",
    "hub_tcrop = './images/crop_train/'\n",
    "hub_vcrop = './images/crop_valid/'\n",
    "tiw_crop = './images/crop_tiw/'\n",
    "\n",
    "lmdb_gt = './lmdb_gt/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. AI허브 데이터셋 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2-1. [한국어 글자체 이미지](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=81)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import natsort\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from PIL import Image\n",
    "from PIL import ImageFile\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 파일 경로, 이미지, 라벨 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open(tiw_label + 'textinthewild_data_info.json', encoding='utf8'))  # json 경로로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.__len__(), data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = dict()\n",
    "img_path_li = []\n",
    "no_img = []\n",
    "\n",
    "for idx, info in tqdm(enumerate(data['images']), total=len(data['images'])):\n",
    "    img_path[data['images'][idx][\"id\"]] = data['images'][idx][\"file_name\"]\n",
    "    img_path_li.append(data['images'][idx][\"file_name\"][:-4])\n",
    "\n",
    "img_path.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "file_path_li = []\n",
    "\n",
    "for (path, dir, files) in os.walk(tiw_img):\n",
    "    for filename in files:\n",
    "        ext = os.path.splitext(filename)[-1]\n",
    "        index_number = filename.split('.')[0]\n",
    "        file_path_li.append(index_number)\n",
    "\n",
    "file_path_li.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# image 파일과 json 파일의 개수가 다르기 때문에 (data, label) 쌍이 맞지 않는 데이터 제거\n",
    "\n",
    "liset = set(file_path_li)\n",
    "li2set = set(img_path_li)\n",
    "black_list = set(liset | li2set) - set(liset & li2set)\n",
    "black_list.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = []\n",
    "text = []\n",
    "bbox = []\n",
    "\n",
    "for idx, info in tqdm(enumerate(data['annotations']), total=len(data['annotations'])):\n",
    "\n",
    "    # label 결측치 제거\n",
    "    if data['annotations'][idx]['text'] == None:\n",
    "        continue\n",
    "    if img_path[data['annotations'][idx]['image_id']][:-4] in black_list:\n",
    "        continue\n",
    "\n",
    "    # 좌표 이상치 제거\n",
    "    if data['annotations'][idx]['bbox'][2] + data['annotations'][idx]['bbox'][0] <= data['annotations'][idx]['bbox'][0]:\n",
    "        continue\n",
    "    if data['annotations'][idx]['bbox'][3] + data['annotations'][idx]['bbox'][1] <= data['annotations'][idx]['bbox'][1]:\n",
    "        continue\n",
    "\n",
    "    # 데이터 추가\n",
    "    path.append(data['annotations'][idx]['image_id'])\n",
    "    text.append(data['annotations'][idx]['text'])\n",
    "    bbox.append(data['annotations'][idx]['bbox'])\n",
    "\n",
    "len(path), len(text), len(bbox)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리(Crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json의 저장 순서는 (x, y, width, height)\n",
    "# PIL의 Image.crop()과 순서는 동일하지만, (width, height)를 크기가 아닌 좌표로 변환하는 작업 필요\n",
    "\n",
    "for i in trange(len(bbox)):\n",
    "    bbox[i][2] = bbox[i][0] + bbox[i][2]\n",
    "    bbox[i][3] = bbox[i][1] + bbox[i][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5  # 이미지 resize 비율\n",
    "starts = 0  # 오류로 중단하기 전 마지막 인덱스\n",
    "\n",
    "for idx, i in tqdm(enumerate(bbox[starts:]), total=len(bbox[starts:])):\n",
    "    idx += starts\n",
    "    image = Image.open(tiw_img + img_path[path[idx]])\n",
    "    crop_image = image.crop(i)\n",
    "\n",
    "    # crop_image.resize((crop_image.size[0] * alpha, crop_image.size[1] * alpha))  # resize\n",
    "    crop_image.save(tiw_crop + f'train_data_{idx}.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 데이터 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_files = list(glob.glob(tiw_crop + '*'))\n",
    "image_files = natsort.natsorted(image_files)  # 크롭된 이미지 리스트 정렬\n",
    "\n",
    "Image.open(image_files[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_new_crop = pd.DataFrame([])\n",
    "csv_new_crop['path'] = image_files\n",
    "csv_new_crop['text'] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_new_crop.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "csv_new_crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lmdb 데이터 생성을 위한 gt 파일 저장\n",
    "csv_new_crop.to_csv(lmdb_gt + 'crop_data.txt', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2-2. [야외 실제 촬영 한글 이미지](https://aihub.or.kr/aihubdata/data/view.do?currMenu=115&topMenu=100&aihubDataSe=realm&dataSetSn=105)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 파일 경로, 이미지, 라벨 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def number_of_subfiles(data_dir):\n",
    "    s = 0\n",
    "    _path = []\n",
    "    _list = []\n",
    "\n",
    "\n",
    "    for (path, dir, files) in os.walk(data_dir):\n",
    "        for filename in files:\n",
    "            ext = os.path.splitext(filename)[-1]\n",
    "\n",
    "            if ext != '.zip':\n",
    "                index_number = filename.split('.')[0]\n",
    "                _list.append(index_number)\n",
    "                _path.append(path + '/' + filename)\n",
    "                s += 1\n",
    "        \n",
    "    _list.sort()\n",
    "\n",
    "    return s, _list, _path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt_img, ti_path, train_img = number_of_subfiles(hub_timg)\n",
    "cnt_label, tl_path, train_label = number_of_subfiles(hub_tlabel)\n",
    "\n",
    "cnt_img, cnt_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liset = set(ti_path)\n",
    "li2set = set(tl_path)\n",
    "black_list = set(liset | li2set) - set(liset & li2set)\n",
    "black_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for b in black_list:\n",
    "    for p in train_label:\n",
    "        if p.split('.')[-2].split('/')[-1] == b:\n",
    "            train_label.remove(p)\n",
    "\n",
    "len(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_img[:50], train_label[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cnt_img, vi_path, valid_img = number_of_subfiles(hub_vimg)\n",
    "cnt_label, vl_path, valid_label = number_of_subfiles(hub_vlabel)\n",
    "\n",
    "cnt_img, cnt_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "valid_img[:50], valid_label[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 이미지, 라벨 리스트 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_json_list = []\n",
    "valid_json_list = []\n",
    "\n",
    "for label in tqdm(train_label):\n",
    "    with open(label, encoding='utf8') as f:\n",
    "        train_json_list.append(json.load(f))\n",
    "\n",
    "for label in tqdm(valid_label):\n",
    "    with open(label, encoding='utf8') as f:\n",
    "        valid_json_list.append(json.load(f))\n",
    "\n",
    "train_json_list.__len__(), valid_json_list.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 데이터 전처리(Crop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def mk_crop(json_list):\n",
    "    idxs = []    # 파일 순서 인덱스\n",
    "    lotate = []  # 이미지 좌표\n",
    "    label = []   # 라벨\n",
    "\n",
    "    for idx, i in tqdm(enumerate(json_list), total=len(json_list)):\n",
    "        for jdx, j in enumerate(i['annotations']):\n",
    "\n",
    "            # 좌표 이상치 제거\n",
    "            if j['bbox'] == [None, None, None, None]:\n",
    "                continue\n",
    "\n",
    "            # 라벨 결측치 제거\n",
    "            elif j['text'] == \"xxx\":\n",
    "                continue\n",
    "\n",
    "            idxs.append(idx)\n",
    "            lotate.append(j['bbox'])\n",
    "            label.append(j['text'])\n",
    "\n",
    "    return idxs, lotate, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_idxs, t_lotate, t_label = mk_crop(train_json_list)\n",
    "v_idxs, v_lotate, v_label = mk_crop(valid_json_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# crop을 위한 좌표 정보 수정\n",
    "for i in trange(len(t_lotate)):\n",
    "    t_lotate[i][2] = t_lotate[i][0] + t_lotate[i][2]\n",
    "    t_lotate[i][3] = t_lotate[i][1] + t_lotate[i][3]\n",
    "\n",
    "for i in trange(len(v_lotate)):\n",
    "    v_lotate[i][2] = v_lotate[i][0] + v_lotate[i][2]\n",
    "    v_lotate[i][3] = v_lotate[i][1] + v_lotate[i][3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### train data 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  # 이미지 resize 비율\n",
    "starts = 0  # 오류로 중단하기 전 마지막 인덱스\n",
    "\n",
    "for idx, i in tqdm(enumerate(t_lotate), total=len(t_lotate)):\n",
    "    idx += starts\n",
    "    image = Image.open(train_img[t_idxs[idx]])\n",
    "    crop_image = image.crop(i)\n",
    "\n",
    "    # crop_image.resize((crop_image.size[0] * alpha, crop_image.size[1] * alpha))  # resize\n",
    "    crop_image.save(hub_tcrop + f'train_data_{str(idx).zfill(7)}.jpg')\n",
    "\n",
    "paths = sorted(glob.glob(hub_tcrop + '/t*.*'))\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['path'] = paths\n",
    "data['label'] = t_label\n",
    "data.to_csv(lmdb_gt + 'hub_tlabel.csv', sep='\\t', encoding='utf8', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### valid data 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "starts = 0\n",
    "\n",
    "for idx, i in tqdm(enumerate(v_lotate), total=len(v_lotate)):\n",
    "    idx += starts\n",
    "    image = Image.open(valid_img[v_idxs[idx]])\n",
    "    crop_image = image.crop(i)\n",
    "\n",
    "    # crop_image.resize((crop_image.size[0] * alpha, crop_image.size[1] * alpha))  # resize\n",
    "    crop_image.save(hub_vcrop + f'valid_data_{str(idx).zfill(7)}.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(v_label)\n",
    "data.to_csv(lmdb_gt + 'hub_vlabel.csv', sep='\\t', encoding='utf8', index=False)\n",
    "\n",
    "label_df = pd.read_csv(lmdb_gt + 'hub_vlabel.csv')\n",
    "label_df.columns = ['label']\n",
    "new_data = {'label': 'xxx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 라벨 데이터가 누락된 행 제거\n",
    "idx = 54104\n",
    "temp1 = label_df[label_df.index < idx]\n",
    "temp2 = label_df[label_df.index >= idx]\n",
    "label_df = temp1.append(new_data, ignore_index=True).append(temp2, ignore_index=True)\n",
    "label_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "paths = sorted(glob.glob(hub_vcrop + '/v*.*'))\n",
    "\n",
    "label_df['path'] = paths\n",
    "\n",
    "label_columns = label_df['label']\n",
    "label_path = label_df['path']\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['path'] = label_path\n",
    "result_df['label'] = label_columns\n",
    "result_df.to_csv('hub_vlabel.csv', sep='\\t', encoding='utf8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Dacon 데이터 전처리 및 lmdb 데이터 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3-1. train.csv 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨링이 잘못되어있거나 누락된 데이터 수정 -> ./open/train_edit.csv 로 저장\n",
    "train_csv_path = 'open/train_edit.csv'\n",
    "train_csv = pd.read_csv(train_csv_path)\n",
    "\n",
    "# lmdb 데이터 생성을 위한 gt 파일 생성\n",
    "train_csv.to_csv('./lmdb_data/train.txt', sep='\\t', header=False, index=False)\n",
    "\n",
    "# 최종 학습 단계의 모델 저장을 위한 임시 validation gt 파일 생성\n",
    "train_csv[:2000].to_csv('./lmdb_data/valid.txt', sep='\\t', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3-2. 학습에 사용할 lmdb 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "window 환경에서 작업 시의 lmdb 파일 생성 명령어:\n",
    "!python ./hallymocr/create_lmdb_dataset.py --inputPath [데이터 root 경로] --gtFile [txt 파일 경로] --outputPath [데이터 저장 경로] --file_size [데이터 총 크기(GB)]\n",
    "\n",
    "linux 환경에서 작업 시의 lmdb 파일 생성 명령어:\n",
    "!python3 ./hallymocr/create_lmdb_dataset.py --inputPath [데이터 root 경로] --gtFile [txt 파일 경로] --outputPath [데이터 저장 경로] --file_size [데이터 총 크기(GB)]\n",
    "'''\n",
    "\n",
    "!python ./hallymocr/create_lmdb_dataset.py --inputPath ./images/crop_tiw --gtFile ./lmdb_data/crop_data.txt --outputPath ./result/tiw --file_size 100\n",
    "!python ./hallymocr/create_lmdb_dataset.py --inputPath ./images/crop_train --gtFile ./lmdb_data/hub_tlabel.txt --outputPath ./result/htrain --file_size 200\n",
    "!python ./hallymocr/create_lmdb_dataset.py --inputPath ./images/crop_valid --gtFile ./lmdb_data/hub_vlabel.txt --outputPath ./result/hvalid --file_size 100\n",
    "!python ./hallymocr/create_lmdb_dataset.py --inputPath ./open/ --gtFile ./lmdb_data/train.txt --outputPath ./result/train --file_size 10\n",
    "!python ./hallymocr/create_lmdb_dataset.py --inputPath ./open/ --gtFile ./lmdb_data/valid.txt --outputPath ./result/valid --file_size 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. OCR 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_txt = ''\n",
    "e = ord('가')\n",
    "\n",
    "for i in range(11172):\n",
    "    ko_txt += chr(e + i)\n",
    "\n",
    "ko_txt += ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import string\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append(\"./hallymocr\")\n",
    "from hallymocr.train import train\n",
    "from hallymocr.test import test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 하이퍼파라미터 설정\n",
    "opt = {\n",
    "    'exp_name': None,\n",
    "    'train_data': './result/',\n",
    "    'valid_data': './result/train',\n",
    "    'manualSeed': 1111,\n",
    "    'workers': 0,\n",
    "    'batch_size': 32,\n",
    "    'num_iter': 100000,\n",
    "    'valInterval': 1000,\n",
    "    'saved_model': '',\n",
    "\n",
    "    'FT': False,\n",
    "    'adam': False,\n",
    "    'lr': 1,\n",
    "    'beta1': 0.9,\n",
    "    'rho': 0.95,\n",
    "    'eps': 1e-8,\n",
    "    'grad_clip': 5,\n",
    "    'baiduCTC': False,\n",
    "    'select_data': 'tiw-htrain-hvalid',\n",
    "    'batch_ratio': '0.3-0.4-0.3',\n",
    "    'total_data_usage_ratio': '1',\n",
    "    'batch_max_length': 15,\n",
    "\n",
    "    'imgH': 256,\n",
    "    'imgW': 256,\n",
    "    'rgb': False,\n",
    "    'character': ko_txt,\n",
    "    'sensitive': False,\n",
    "    'PAD': False,\n",
    "    'data_filtering_off': False,\n",
    "    'Transformation': 'TPS',  # None|TPS\n",
    "    'FeatureExtraction': 'ResNet',  # VGG|ResNet|RCNN\n",
    "    'SequenceModeling': 'BiLSTM',  # None|BiLSTM\n",
    "    'Prediction': 'Attn',  # CTC|Attn\n",
    "    'num_fiducial': 20,\n",
    "    'input_channel': 1,\n",
    "    'output_channel': 512,\n",
    "    'hidden_size': 256,\n",
    "}\n",
    "\n",
    "# 모델 추가 세부사항 설정\n",
    "if not opt['exp_name']:\n",
    "    opt['exp_name'] = '{Transformation}-{FeatureExtraction}-{SequenceModeling}-{Prediction}'.format(**opt)\n",
    "    opt['exp_name'] += '-Seed{manualSeed}'.format(**opt)\n",
    "    # print(opt.exp_name)\n",
    "\n",
    "os.makedirs('./saved_models/{exp_name}'.format(**opt), exist_ok=True)\n",
    "\n",
    "\"\"\" Seed and GPU setting \"\"\"\n",
    "random.seed(opt['manualSeed'])\n",
    "np.random.seed(opt['manualSeed'])\n",
    "torch.manual_seed(opt['manualSeed'])\n",
    "torch.cuda.manual_seed(opt['manualSeed'])\n",
    "\n",
    "cudnn.benchmark = True\n",
    "cudnn.deterministic = True\n",
    "opt['num_gpu'] = torch.cuda.device_count()\n",
    "\n",
    "if opt['num_gpu'] > 1:\n",
    "    print('------ Use multi-GPU setting ------')\n",
    "    print('if you stuck too long time with multi-GPU setting, try to set --workers 0')\n",
    "    opt['workers'] = opt['workers'] * opt['num_gpu']\n",
    "    opt['batch_size'] = opt['batch_size'] * opt['num_gpu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dacon 데이터로 2차 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "opt['exp_name'] = '{Transformation}-{FeatureExtraction}-{SequenceModeling}-{Prediction}-Seed{manualSeed}'.format(**opt)\n",
    "opt['saved_model'] = 'saved_models/{Transformation}-{FeatureExtraction}-{SequenceModeling}-{Prediction}-Seed{manualSeed}/best_accuracy.pth'.format(**opt)\n",
    "opt['num_iter'] = 50000\n",
    "opt['lr'] = 0.001\n",
    "opt['select_data'] = 'train'\n",
    "opt['batch_ratio'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 최종 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 모델 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 저장된 모델 load\n",
    "opt['saved_model'] = 'saved_models/{Transformation}-{FeatureExtraction}-{SequenceModeling}-{Prediction}-Seed{manualSeed}/best_accuracy.pth'.format(**opt)\n",
    "opt['test_data'] = './open/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result = test(opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV 파일 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "result_csv_path = 'open/sample_submission.csv'\n",
    "result_csv = pd.read_csv(result_csv_path)\n",
    "\n",
    "result_csv['text'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "time = datetime.now().strftime('%m%d_%H%M')\n",
    "csv_name = f'{time}.csv'\n",
    "result_csv.to_csv(csv_name, index=False, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(csv_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('ocr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "31727eb632e7da8a5b80cec6e557fc09a461fb488b9fb3204718db5f91f70f0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
